# Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j

# Weaviate Configuration
WEAVIATE_URL=http://localhost:8080
WEAVIATE_API_KEY=
WEAVIATE_ADDITIONAL_HEADERS=

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:12b-it-qat
OLLAMA_TIMEOUT=30

# Multi-Provider LLM Configuration
# Active LLM Provider Selection (choose one):
# - ollama      (local, free, no API key required)
# - openrouter  (cloud, paid, multiple models)
# - gemini      (cloud, paid, Google AI)
# - anthropic   (cloud, paid, Claude models)
# - openai      (cloud, paid, GPT models)
SELECTED_LLM_PROVIDER=ollama

# Active Model Selection (optional - uses provider default if not set):
# For Ollama: llama2, gemma3:12b-it-qat, etc.
# For OpenRouter: meta-llama/llama-2-70b-chat, etc.
# For Gemini: gemini-pro, gemini-1.5-pro, etc.
# For Anthropic: claude-3-sonnet, claude-3-opus, etc.
# For OpenAI: gpt-4, gpt-3.5-turbo, etc.
SELECTED_LLM_MODEL=

# Knowledge Graph Extraction LLM Configuration
# (Use more powerful models for better philosophical relationship extraction)
#
# LLM MODEL COMPARISON TABLE FOR KNOWLEDGE GRAPH EXTRACTION
# =========================================================
# Model                      | Performance & Reasoning                  | Cost (Input) $/1M tokens        | Best for Your Task?
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# Gemini 2.0 Pro            | State-of-the-art for complex reasoning. | $1.25 (up to 200k)              | Yes, for R&D. Ideal for high-quality,
#                            | Handles long, nuanced philosophical     | $2.50 (above 200k tokens)       | complex relationship extraction where
#                            | texts exceptionally well.               |                                  | accuracy is paramount.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# Gemini 2.5 Flash          | Balances performance and efficiency.     | $0.30 / $2.50                   | Yes, for production. Strong contender
#                            | Great for general tasks and real-time   |                                  | for cost-effective, high-throughput
#                            | applications.                            |                                  | extraction of simpler relationships.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# Claude 4 Sonnet            | Strong performance and instruction-      | $3.00 (up to 200k)              | Yes, as a strong alternative. Excellent
#                            | following. A reliable "workhorse" model.| $6.00 (above 200k tokens)       | balance of performance and cost.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# OpenAI GPT-5               | The new state-of-the-art. Excels at     | $1.25 / $10.00                  | Yes, for peak performance. Best model
#                            | complex, multi-step reasoning.          |                                  | for pure performance on complex tasks.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# OpenAI GPT-5-mini          | Excellent. A "sweet spot" model.        | $0.25 / $2.00                   | Yes, highly recommended. Best overall
#                            | Outperforms many previous-gen models    |                                  | choice for balancing performance and
#                            | on reasoning, math, and coding.         |                                  | cost. Intelligent enough for complex
#                            |                                          |                                  | philosophical relationships.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# OpenAI GPT-5-nano          | Good for simple tasks like              | $0.05 / $0.40                   | Yes, for simple tasks. Ideal for
#                            | summarization and classification.       |                                  | high-volume processing where
#                            | Performance drops on complex reasoning. |                                  | relationships are straightforward.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# Ollama (Local)             |                                          |                                  |
# gemma3:12b-it-qat          | Good performance for local processing.  | FREE (local inference)          | Yes, for privacy/cost. Excellent for
#                            | Suitable for philosophical reasoning.    | Hardware: ~12GB RAM required     | sensitive content or budget-conscious
#                            |                                          |                                  | development with reasonable quality.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# OpenRouter (Cloud)         |                                          |                                  |
# deepseek/deepseek-chat-v3.1| Strong reasoning capabilities with       | FREE (community tier)           | Yes, for budget development. Good
#                            | competitive performance.                |                                  | balance of quality and cost for
#                            |                                          |                                  | initial development and testing.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# moonshotai/kimi-k2         | Decent performance with long context    | FREE (community tier)           | Maybe, for basic extraction. Suitable
#                            | window. Good for document analysis.     |                                  | for simple relationship extraction
#                            |                                          |                                  | with long philosophical texts.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# qwen/qwen3-235b-a22b       | Large parameter model with strong       | FREE (community tier)           | Yes, for complex reasoning. Strong
#                            | reasoning capabilities.                  |                                  | performance for complex philosophical
#                            |                                          |                                  | relationship extraction at no cost.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
# Gemma 3 27B                | High. An extremely capable open-source  | $0.05 / $0.10 (self-hosted or   | Yes, for cost-sensitive projects or
#                            | model. Excels at complex reasoning and  | through providers like Together   | projects with privacy requirements.
#                            | code. Can approach or exceed performance| AI/OpenRouter). 128K context.    | Top-tier contender for "main workhorse"
#                            | of some closed-source "lite" models.   |                                  | role. Phenomenal performance-to-cost ratio.
# ---------------------------|------------------------------------------|-----------------------------------|-------------------
#
# RECOMMENDATION SUMMARY:
# - For Production: Gemini 2.5 Flash or GPT-5-mini (best cost/performance balance)
# - For R&D/Accuracy: Gemini 2.0 Pro or Claude 4 Sonnet (highest quality)
# - For Budget/Privacy: Ollama gemma3:12b-it-qat (free, local, privacy-preserving)
# - For Development: OpenRouter free models (qwen3-235b-a22b recommended for complex reasoning)
#
KG_LLM_PROVIDER=anthropic
KG_LLM_MODEL=claude-3-sonnet-20240229

# LLM Provider API Keys (only needed for cloud providers)
OPENROUTER_API_KEY=
GEMINI_API_KEY=
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
VOYAGE_API_KEY=

# LLM Configuration
LLM_MAX_TOKENS=4000
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=30
LLM_RETRY_ATTEMPTS=3

# LangChain Configuration
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=arete

# Application Configuration
APP_NAME=Arete Philosophy Tutor
APP_VERSION=0.1.0
DEBUG=true
LOG_LEVEL=INFO
MAX_CONTEXT_LENGTH=5000
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Embedding Configuration
# Available models (ranked by quality for philosophical texts):
#
# BEST QUALITY (Recommended for production):
# - paraphrase-multilingual-mpnet-base-v2    (768 dims, multilingual, Classical Greek/Latin support)
#
# HIGH QUALITY:
# - all-mpnet-base-v2                        (768 dims, English-only, fastest high-quality)
# 
# BALANCED (Good for development/testing):
# - paraphrase-multilingual-MiniLM-L12-v2    (384 dims, multilingual, 3x faster)
#
# FASTEST (For quick prototyping):
# - all-MiniLM-L6-v2                         (384 dims, English-only, 5x faster)
#
# STATE-OF-THE-ART via Ollama (requires 'ollama pull' first):
# - dengcao/qwen3-embedding-8b:q8_0           (8192 dims, MTEB leaderboard #1, 8GB RAM)
# - qwen3-embedding-8b                        (8192 dims, alternative name)
#
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=paraphrase-multilingual-mpnet-base-v2
# EMBEDDING_MODEL=dengcao/qwen3-embedding-8b:q8_0  # Uncomment for SOTA quality
EMBEDDING_DIMENSION=768
MAX_EMBEDDING_BATCH_SIZE=32

# Retrieval Configuration
DENSE_WEIGHT=0.7
SPARSE_WEIGHT=0.3
TOP_K_PASSAGES=5
MAX_GRAPH_DEPTH=2

# UI Configuration
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=localhost
STREAMLIT_SERVER_HEADLESS=false

# File Processing Configuration
MAX_FILE_SIZE_MB=100
SUPPORTED_FORMATS=pdf,txt,md,xml,tei
PDF_EXTRACTION_METHOD=pymupdf4llm

# Security Configuration
SECRET_KEY=your-secret-key-here
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8501

# Performance Configuration
WORKER_PROCESSES=4
CACHE_TTL_SECONDS=3600
MAX_CONCURRENT_REQUESTS=10

# Development Configuration
PYTEST_ADDOPTS=--cov=arete --cov-report=html
PYTHONPATH=src