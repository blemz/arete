# Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j

# Weaviate Configuration
WEAVIATE_URL=http://localhost:8080
WEAVIATE_API_KEY=
WEAVIATE_ADDITIONAL_HEADERS=

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:12b-it-qat
OLLAMA_TIMEOUT=30

# Multi-Provider LLM Configuration
# Active LLM Provider Selection (choose one):
# - ollama      (local, free, no API key required)
# - openrouter  (cloud, paid, multiple models)
# - gemini      (cloud, paid, Google AI)
# - anthropic   (cloud, paid, Claude models)
# - openai      (cloud, paid, GPT models)
SELECTED_LLM_PROVIDER=ollama

# Active Model Selection (optional - uses provider default if not set):
# For Ollama: llama2, gemma3:12b-it-qat, etc.
# For OpenRouter: meta-llama/llama-2-70b-chat, etc.
# For Gemini: gemini-pro, gemini-1.5-pro, etc.
# For Anthropic: claude-3-sonnet, claude-3-opus, etc.
# For OpenAI: gpt-4, gpt-3.5-turbo, etc.
SELECTED_LLM_MODEL=

# Knowledge Graph Extraction LLM Configuration
# (Use more powerful models for better philosophical relationship extraction)
# Recommended models for knowledge graph extraction:
# - ollama: gemma3:12b-it-qat (larger model for better accuracy)
# - anthropic: claude-3-sonnet (excellent for complex reasoning) 
# - openai: gpt-4 (best for nuanced philosophical relationships)
# - gemini: gemini-1.5-pro (good balance of speed and accuracy)
# - openrouter: meta-llama/llama-3-70b-instruct (powerful for complex extraction)
KG_LLM_PROVIDER=anthropic
KG_LLM_MODEL=claude-3-sonnet-20240229

# LLM Provider API Keys (only needed for cloud providers)
OPENROUTER_API_KEY=
GEMINI_API_KEY=
ANTHROPIC_API_KEY=
OPENAI_API_KEY=

# LLM Configuration
LLM_MAX_TOKENS=4000
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=30
LLM_RETRY_ATTEMPTS=3

# LangChain Configuration
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=arete

# Application Configuration
APP_NAME=Arete Philosophy Tutor
APP_VERSION=0.1.0
DEBUG=true
LOG_LEVEL=INFO
MAX_CONTEXT_LENGTH=5000
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Embedding Configuration
# Available models (ranked by quality for philosophical texts):
#
# BEST QUALITY (Recommended for production):
# - paraphrase-multilingual-mpnet-base-v2    (768 dims, multilingual, Classical Greek/Latin support)
#
# HIGH QUALITY:
# - all-mpnet-base-v2                        (768 dims, English-only, fastest high-quality)
# 
# BALANCED (Good for development/testing):
# - paraphrase-multilingual-MiniLM-L12-v2    (384 dims, multilingual, 3x faster)
#
# FASTEST (For quick prototyping):
# - all-MiniLM-L6-v2                         (384 dims, English-only, 5x faster)
#
# STATE-OF-THE-ART via Ollama (requires 'ollama pull' first):
# - dengcao/qwen3-embedding-8b:q8_0           (8192 dims, MTEB leaderboard #1, 8GB RAM)
# - qwen3-embedding-8b                        (8192 dims, alternative name)
#
EMBEDDING_MODEL=paraphrase-multilingual-mpnet-base-v2
# EMBEDDING_MODEL=dengcao/qwen3-embedding-8b:q8_0  # Uncomment for SOTA quality
EMBEDDING_DIMENSION=768
MAX_EMBEDDING_BATCH_SIZE=32

# Retrieval Configuration
DENSE_WEIGHT=0.7
SPARSE_WEIGHT=0.3
TOP_K_PASSAGES=5
MAX_GRAPH_DEPTH=2

# UI Configuration
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=localhost
STREAMLIT_SERVER_HEADLESS=false

# File Processing Configuration
MAX_FILE_SIZE_MB=100
SUPPORTED_FORMATS=pdf,txt,md,xml,tei
PDF_EXTRACTION_METHOD=pymupdf4llm

# Security Configuration
SECRET_KEY=your-secret-key-here
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8501

# Performance Configuration
WORKER_PROCESSES=4
CACHE_TTL_SECONDS=3600
MAX_CONCURRENT_REQUESTS=10

# Development Configuration
PYTEST_ADDOPTS=--cov=arete --cov-report=html
PYTHONPATH=src