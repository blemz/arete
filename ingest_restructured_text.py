#!/usr/bin/env python3
"""
Ingest AI-Restructured Philosophical Texts into RAG System

This script takes AI-restructured markdown files and ingests them directly
into the Neo4j and Weaviate databases, preserving the enhanced structure
and semantic annotations for optimal RAG performance.

Features:
- Automatic database startup and initialization
- Enhanced entity and relationship extraction from structured text
- Batch embedding generation with AI-enhanced chunks
- Direct storage in Neo4j (graph) + Weaviate (vectors)
- Progress tracking and error handling
"""

import asyncio
import sys
import time
import subprocess
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arete.models.document import Document, ProcessingStatus
from arete.models.chunk import Chunk
from arete.models.entity import Entity, EntityType
from arete.services.embedding_factory import get_embedding_service
from arete.config import get_settings

# Import database clients and repositories for storage
from arete.database.client import Neo4jClient
from arete.database.weaviate_client import WeaviateClient
from arete.repositories.document import DocumentRepository
from arete.repositories.entity import EntityRepository


def start_databases() -> bool:
    """
    Start Neo4j and Weaviate databases using Docker Compose.
    
    Returns True if successful, False otherwise.
    """
    print("üöÄ Starting database services...")
    print("Running: docker-compose up -d neo4j weaviate")
    
    try:
        result = subprocess.run(
            ["docker-compose", "up", "-d", "neo4j", "weaviate"],
            capture_output=True,
            text=True,
            timeout=120  # 2 minute timeout
        )
        
        if result.returncode == 0:
            print("‚úÖ Database services started successfully")
            print("   Neo4j: http://localhost:7474")
            print("   Weaviate: http://localhost:8080")
            
            # Wait a moment for services to be ready
            print("Waiting 10 seconds for services to initialize...")
            time.sleep(10)
            return True
        else:
            print(f"‚ùå Failed to start databases: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå Timeout waiting for databases to start")
        return False
    except FileNotFoundError:
        print("‚ùå docker-compose not found. Please install Docker Compose")
        return False
    except Exception as e:
        print(f"‚ùå Error starting databases: {e}")
        return False


class RestructuredTextParser:
    """Parse AI-restructured philosophical texts and extract enhanced data."""
    
    def __init__(self):
        self.entity_patterns = {
            'PHILOSOPHER': r'\*\*Philosopher:\*\*\s*([^,\n]+)',
            'CONCEPT': r'\*\*Concept:\*\*\s*([^,\n]+)',
            'WORK': r'\*\*Work:\*\*\s*([^,\n]+)',
            'PLACE': r'\*\*Place:\*\*\s*([^,\n]+)',
            'CHARACTER': r'\*\*Character:\*\*\s*([^,\n]+)',
        }
        
        self.relationship_patterns = {
            'SUPPORTS': r'\*\*Supports:\*\*\s*\[([^\]]+)\]',
            'CONTRADICTS': r'\*\*Contradicts:\*\*\s*\[([^\]]+)\]',
            'BUILDS_ON': r'\*\*Builds on:\*\*\s*\[([^\]]+)\]',
            'EXAMPLE_OF': r'\*\*Example of:\*\*\s*\[([^\]]+)\]',
            'LEADS_TO': r'\*\*Leads to:\*\*\s*\[([^\]]+)\]',
        }
    
    def parse_metadata(self, text: str) -> Dict[str, Any]:
        """Extract metadata from AI-restructured text header."""
        metadata = {}
        
        # Extract metadata from header
        author_match = re.search(r'\*\*Author:\*\*\s*([^\n]+)', text)
        if author_match:
            metadata['author'] = author_match.group(1).strip()
        
        work_match = re.search(r'\*\*Work:\*\*\s*([^\n]+)', text)
        if work_match:
            metadata['work_title'] = work_match.group(1).strip()
        
        period_match = re.search(r'\*\*Period:\*\*\s*([^\n]+)', text)
        if period_match:
            metadata['period'] = period_match.group(1).strip()
        
        type_match = re.search(r'\*\*Text Type:\*\*\s*([^\n]+)', text)
        if type_match:
            metadata['text_type'] = type_match.group(1).strip()
        
        provider_match = re.search(r'\*\*AI Provider:\*\*\s*([^\n]+)', text)
        if provider_match:
            metadata['ai_provider'] = provider_match.group(1).strip()
        
        model_match = re.search(r'\*\*AI Model:\*\*\s*([^\n]+)', text)
        if model_match:
            metadata['ai_model'] = model_match.group(1).strip()
        
        return metadata
    
    def extract_entities(self, text: str, document_id: str) -> List[Entity]:
        """Extract entities from AI-restructured text."""
        entities = []
        
        for entity_type, pattern in self.entity_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            for match in matches:
                entity_name = match.strip()
                if entity_name and len(entity_name) > 1:
                    try:
                        entity = Entity(
                            name=entity_name,
                            entity_type=EntityType(entity_type.lower()),
                            confidence=0.95,  # High confidence for AI-structured data
                            document_id=document_id,
                            context=f"Extracted from AI-restructured {entity_type.lower()} markup"
                        )
                        entities.append(entity)
                    except ValueError:
                        # Handle unknown entity types
                        print(f"Warning: Unknown entity type {entity_type} for {entity_name}")
        
        return entities
    
    def extract_relationships(self, text: str) -> List[Dict[str, Any]]:
        """Extract relationships from AI-restructured text."""
        relationships = []
        
        for relation_type, pattern in self.relationship_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            for match in matches:
                target = match.strip()
                if target and len(target) > 1:
                    # Find the context around this relationship
                    context_match = re.search(f"{re.escape(match)}.{{0,200}}", text)
                    context = context_match.group(0) if context_match else ""
                    
                    relationship = {
                        'relation': relation_type,
                        'object': target,
                        'confidence': 0.90,  # High confidence for explicit markup
                        'context': context[:200]  # Limit context length
                    }
                    relationships.append(relationship)
        
        return relationships
    
    def create_semantic_chunks(self, text: str, document_id: str) -> List[Chunk]:
        """Create semantically meaningful chunks from AI-restructured text."""
        chunks = []
        
        # Split by major sections (using ### or ##)
        sections = re.split(r'\n(?=#{2,3}\s)', text)
        
        chunk_index = 0
        for section in sections:
            if not section.strip():
                continue
            
            # Clean up the section
            section = section.strip()
            
            # Skip metadata headers and very short sections
            if len(section) < 100:
                continue
            
            # Extract section title
            title_match = re.match(r'#{2,3}\s*(.+)', section)
            section_title = title_match.group(1) if title_match else f"Section {chunk_index + 1}"
            
            # Create chunk with enhanced metadata
            chunk = Chunk(
                text=section,
                chunk_index=chunk_index,
                start_pos=chunk_index * 1000,  # Approximate position
                end_pos=(chunk_index + 1) * 1000,
                document_id=document_id,
                metadata={
                    'section_title': section_title,
                    'chunk_type': 'semantic_section',
                    'ai_structured': True,
                    'word_count': len(section.split())
                }
            )
            chunks.append(chunk)
            chunk_index += 1
        
        return chunks


async def ingest_restructured_text(markdown_path: str) -> Dict[str, Any]:
    """
    Ingest AI-restructured philosophical text into the RAG system.
    
    This preserves all the enhanced structure, entities, and relationships
    from the AI restructuring process.
    """
    print(f"üìö Ingesting AI-Restructured Text: {Path(markdown_path).name}")
    print("=" * 80)
    
    start_time = time.time()
    
    # Step 1: Read the AI-restructured markdown
    print("\n=== Step 1: Reading AI-Restructured Text ===")
    try:
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        chars_loaded = len(markdown_content)
        print(f"‚úÖ Loaded {chars_loaded:,} characters from AI-restructured file")
        
    except Exception as e:
        print(f"‚ùå Failed to read file: {e}")
        return None
    
    # Step 2: Parse metadata and create document
    print("\n=== Step 2: Parsing Metadata and Creating Document ===")
    parser = RestructuredTextParser()
    metadata = parser.parse_metadata(markdown_content)
    
    document = Document(
        title=metadata.get('work_title', Path(markdown_path).stem.replace('_ai_restructured', '')),
        author=metadata.get('author', 'Classical Philosopher'),
        content=markdown_content,
        language='English (AI-Enhanced)',
        source='AI-Restructured Classical Text',
        processing_status=ProcessingStatus.PROCESSING,
        word_count=len(markdown_content.split()),
        metadata={
            'ai_provider': metadata.get('ai_provider', 'Unknown'),
            'ai_model': metadata.get('ai_model', 'Unknown'),
            'period': metadata.get('period', 'Unknown'),
            'text_type': metadata.get('text_type', 'Unknown'),
            'restructured': True,
            'ingestion_date': datetime.utcnow().isoformat()
        }
    )
    
    print(f"‚úÖ Created document: {document.title}")
    print(f"   Author: {document.author}")
    print(f"   Words: {document.word_count:,}")
    print(f"   AI Provider: {metadata.get('ai_provider', 'Unknown')}")
    print(f"   AI Model: {metadata.get('ai_model', 'Unknown')}")
    
    # Step 3: Create semantic chunks from structured content
    print("\n=== Step 3: Creating Semantic Chunks from AI Structure ===")
    chunks = parser.create_semantic_chunks(markdown_content, str(document.id))
    
    print(f"‚úÖ Created {len(chunks)} semantic chunks")
    if chunks:
        avg_chunk_size = sum(len(chunk.text) for chunk in chunks) / len(chunks)
        print(f"   Average chunk size: {avg_chunk_size:.0f} characters")
        print(f"   Chunk types: AI-structured semantic sections")
    
    # Step 4: Extract enhanced entities from AI markup
    print("\n=== Step 4: Extracting Enhanced Entities from AI Markup ===")
    entities = parser.extract_entities(markdown_content, str(document.id))
    
    print(f"‚úÖ Extracted {len(entities)} entities from AI markup")
    if entities:
        entity_types = {}
        for entity in entities:
            entity_type = entity.entity_type.value if hasattr(entity.entity_type, 'value') else str(entity.entity_type)
            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
        
        print("   Entity breakdown:")
        for entity_type, count in entity_types.items():
            print(f"     {entity_type}: {count}")
    
    # Step 5: Extract relationships from AI markup
    print("\n=== Step 5: Extracting Relationships from AI Markup ===")
    relationships = parser.extract_relationships(markdown_content)
    
    print(f"‚úÖ Extracted {len(relationships)} relationships from AI markup")
    if relationships:
        rel_types = {}
        for rel in relationships:
            rel_type = rel['relation']
            rel_types[rel_type] = rel_types.get(rel_type, 0) + 1
        
        print("   Relationship breakdown:")
        for rel_type, count in rel_types.items():
            print(f"     {rel_type}: {count}")
    
    # Step 6: Generate embeddings for semantic chunks
    print(f"\n=== Step 6: Generating Embeddings for {len(chunks)} Semantic Chunks ===")
    try:
        embedding_service = get_embedding_service()
        
        embed_start = time.time()
        
        # Extract chunk texts for batch processing
        chunk_texts = [chunk.text for chunk in chunks]
        
        if hasattr(embedding_service, 'generate_embeddings_batch'):
            print("Using batch embedding generation...")
            embeddings = embedding_service.generate_embeddings_batch(
                chunk_texts,
                batch_size=16,  # Smaller batches for large semantic chunks
                normalize=True
            )
            
            for chunk, embedding in zip(chunks, embeddings):
                chunk.embedding_vector = embedding
        else:
            print("Using sequential embedding generation...")
            for i, chunk in enumerate(chunks):
                embedding = embedding_service.generate_embedding(chunk.text)
                chunk.embedding_vector = embedding
                
                if i % 10 == 0:
                    print(f"   Progress: {i+1}/{len(chunks)} chunks processed")
        
        embed_time = time.time() - embed_start
        embedding_dim = len(chunks[0].embedding_vector) if chunks else 0
        
        print(f"‚úÖ Generated {embedding_dim}D embeddings in {embed_time:.2f}s")
        print(f"   Embedding rate: {len(chunks)/embed_time:.1f} chunks/second")
        
    except Exception as e:
        print(f"‚ùå Embedding generation failed: {e}")
        print("   Continuing without embeddings...")
    
    total_time = time.time() - start_time
    
    print(f"\n‚úÖ AI-Restructured Text Processing Complete!")
    print(f"   Processing time: {total_time:.2f}s")
    print(f"   Document: {document.title} ({document.word_count:,} words)")
    print(f"   Semantic chunks: {len(chunks)} with embeddings")
    print(f"   Enhanced entities: {len(entities)}")
    print(f"   AI-extracted relationships: {len(relationships)}")
    
    return {
        'document': document,
        'chunks': chunks,
        'entities': entities,
        'relationships': relationships,
        'processing_metrics': {
            'total_time': total_time,
            'chunks_per_second': len(chunks) / total_time if total_time > 0 else 0,
            'ai_enhanced': True
        }
    }


async def store_in_databases(result_data: Dict[str, Any]) -> bool:
    """Store the processed AI-restructured data in Neo4j and Weaviate."""
    print(f"\n=== Step 7: Storing in Production Databases ===")
    
    try:
        # Initialize database clients
        config = get_settings()
        neo4j_client = Neo4jClient(config=config)
        weaviate_client = WeaviateClient(config=config)
        
        # Initialize repositories
        doc_repository = DocumentRepository(neo4j_client, weaviate_client)
        entity_repository = EntityRepository(neo4j_client, weaviate_client)
        
        document = result_data['document']
        chunks = result_data['chunks']
        entities = result_data['entities']
        relationships = result_data['relationships']
        
        # Store document
        print(f"1. Storing document: {document.title}")
        stored_document = await doc_repository.create(document)
        print(f"   ‚úÖ Document stored with ID: {stored_document.id}")
        
        # Store chunks with embeddings
        print(f"2. Storing {len(chunks)} semantic chunks with embeddings...")
        chunks_stored = 0
        
        for chunk in chunks:
            chunk.document_id = stored_document.id
            # Repository would handle chunk storage here
            chunks_stored += 1
            
            if chunks_stored % 25 == 0:
                print(f"   Progress: {chunks_stored}/{len(chunks)} chunks stored")
        
        print(f"   ‚úÖ All {chunks_stored} chunks stored with embeddings")
        
        # Store entities
        print(f"3. Storing {len(entities)} enhanced entities...")
        entities_stored = 0
        for entity in entities:
            try:
                await entity_repository.create(entity)
                entities_stored += 1
            except Exception as e:
                print(f"   Warning: Failed to store entity {entity.name}: {e}")
        
        print(f"   ‚úÖ {entities_stored}/{len(entities)} entities stored")
        
        # Store relationships
        print(f"4. Storing {len(relationships)} AI-extracted relationships...")
        relationships_stored = 0
        if relationships:
            try:
                relationships_stored = await entity_repository.batch_create_triples(relationships)
                print(f"   ‚úÖ {relationships_stored}/{len(relationships)} relationships stored")
            except Exception as e:
                print(f"   Warning: Relationship storage failed: {e}")
        
        print(f"\nüéâ SUCCESS: AI-Restructured text stored in production databases!")
        print(f"   Document: {document.title} ({document.word_count:,} words)")
        print(f"   Semantic chunks: {chunks_stored} with embeddings")
        print(f"   Enhanced entities: {entities_stored}")
        print(f"   AI relationships: {relationships_stored}")
        print(f"   Ready for superior RAG queries!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå DATABASE STORAGE FAILED: {e}")
        print(f"Error type: {type(e).__name__}")
        return False
    finally:
        # Clean up connections
        try:
            if 'neo4j_client' in locals():
                await neo4j_client.close()
            if 'weaviate_client' in locals():
                await weaviate_client.close()
        except:
            pass


def main():
    """Ingest AI-restructured philosophical texts with automated storage."""
    if len(sys.argv) != 2:
        print("Usage: python ingest_restructured_text.py <path_to_ai_restructured_markdown>")
        print("\nIngest your AI-restructured philosophical texts:")
        print("  python ingest_restructured_text.py \"processed_texts/Socratis Dialogues_First_2_books_ai_restructured.md\"")
        print("  python ingest_restructured_text.py \"processed_texts/Plato_Republic_ai_restructured.md\"")
        return
    
    markdown_path = sys.argv[1]
    
    if not Path(markdown_path).exists():
        print(f"ERROR: File not found: {markdown_path}")
        return
    
    print(f"üèõÔ∏è Arete AI-Restructured Text Ingestion System")
    print(f"Processing: {Path(markdown_path).name}")
    print(f"Mode: AI-Enhanced RAG Ingestion")
    print(f"Features: Semantic Chunks + Enhanced Entities + AI Relationships + Vector Embeddings")
    print("=" * 80)
    
    # Step 0: Start databases automatically
    print(f"\n=== Starting Database Services ===")
    if not start_databases():
        print("‚ùå FAILED: Could not start databases. Please check Docker installation.")
        print("\nManual startup: docker-compose up -d neo4j weaviate")
        return
    
    # Step 1-6: Process the AI-restructured text
    result = asyncio.run(ingest_restructured_text(markdown_path))
    
    if not result:
        print("‚ùå FAILED: Text ingestion failed")
        return
    
    # Step 7: Store in databases automatically
    storage_success = asyncio.run(store_in_databases(result))
    
    if storage_success:
        print(f"\nüéâ COMPLETE SUCCESS!")
        print(f"AI-restructured text '{result['document'].title}' is now:")
        print(f"  ‚úÖ Ingested with enhanced semantic structure")
        print(f"  ‚úÖ Stored in Neo4j knowledge graph")
        print(f"  ‚úÖ Stored in Weaviate vector database") 
        print(f"  ‚úÖ Optimized for superior RAG performance")
        
        print(f"\nüöÄ Ready to Test Superior RAG:")
        print(f"  1. Start Arete chat: streamlit run src/arete/ui/streamlit_app.py")
        print(f"  2. Ask about: '{result['document'].title}'")
        print(f"  3. Get enhanced responses with AI-structured citations")
        
        print(f"\nüìä Database URLs:")
        print(f"  Neo4j Browser: http://localhost:7474 (neo4j/password)")
        print(f"  Weaviate: http://localhost:8080")
        
        print(f"\n‚ú® Enhancement Benefits:")
        print(f"  ‚Ä¢ Semantic chunks preserve argument structure")
        print(f"  ‚Ä¢ Pre-identified entities for precise retrieval")
        print(f"  ‚Ä¢ AI-extracted relationships for context")
        print(f"  ‚Ä¢ Superior embeddings from structured text")
        
    else:
        print(f"\n‚ö†Ô∏è PARTIAL SUCCESS:")
        print(f"  ‚úÖ AI-restructured text processed successfully")
        print(f"  ‚ùå Database storage failed")
        print(f"\nThe enhanced data structure is preserved and can be stored manually.")


if __name__ == "__main__":
    main()