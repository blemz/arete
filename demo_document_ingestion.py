#!/usr/bin/env python3
"""
Demo of using the existing Arete data ingestion pipeline properly.

This demonstrates the correct way to use the existing Phase 2 infrastructure
that's already 97% complete, rather than creating new scripts.
"""

import asyncio
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arete.processing.extractors import PDFExtractor
from arete.processing.chunker import ChunkingStrategy
from arete.pipelines.kg_extraction import run_kg_extraction  
from arete.models.document import Document, ProcessingStatus
from arete.models.chunk import Chunk
from arete.services.embedding_service import EmbeddingService
from arete.repositories.document import DocumentRepository
from arete.repositories.embedding import EmbeddingRepository
from arete.database.neo4j_client import Neo4jClient
from arete.database.weaviate_client import WeaviateClient
from arete.config import get_config

async def demonstrate_complete_pipeline(pdf_path: str):
    """
    Demonstrate the complete data ingestion pipeline using existing infrastructure.
    
    This shows how the existing Phase 2 pipeline should work:
    1. PDF Extraction (Phase 2.1) âœ…
    2. Document Creation (Phase 2.1) âœ…  
    3. Text Chunking (Phase 2.1) âœ…
    4. Knowledge Graph Extraction (Phase 2.2) âœ…
    5. Embedding Generation (Phase 2.3) âœ…
    6. Storage in dual databases âœ…
    """
    print(f"ğŸ›ï¸ Arete Complete Pipeline Demo: {pdf_path}")
    print("=" * 60)
    
    # Initialize configuration
    config = get_config()
    
    # Step 1: PDF Extraction using existing PDFExtractor âœ…
    print("\nğŸ“„ Step 1: PDF Text Extraction")
    pdf_extractor = PDFExtractor()
    try:
        extraction_result = pdf_extractor.extract_from_file(pdf_path)
        print(f"âœ… Extracted {len(extraction_result['text'])} characters")
        print(f"ğŸ“Š Metadata: {extraction_result['metadata'].title} by {extraction_result['metadata'].author}")
    except Exception as e:
        print(f"âŒ PDF extraction failed: {e}")
        return
    
    # Step 2: Document Model Creation âœ…
    print("\nğŸ“š Step 2: Document Model Creation")  
    document = Document(
        title=extraction_result['metadata'].title or Path(pdf_path).stem,
        author=extraction_result['metadata'].author or "Unknown",
        content=extraction_result['text'],
        language="Ancient Greek (English Translation)",
        source="Classical Philosophy Collection",
        processing_status=ProcessingStatus.PROCESSING,
        word_count=len(extraction_result['text'].split())
    )
    print(f"âœ… Created document: {document.title} by {document.author}")
    print(f"ğŸ“Š Word count: {document.word_count:,}")
    
    # Step 3: Text Chunking using existing ChunkingStrategy âœ…
    print("\nğŸª“ Step 3: Intelligent Text Chunking")
    chunker = ChunkingStrategy.get_chunker("paragraph")  # Using existing factory
    raw_chunks = chunker.chunk_text(document.content)
    
    # Create Chunk models
    chunks = []
    for i, chunk_text in enumerate(raw_chunks[:5]):  # Limit to 5 for demo
        chunk = Chunk(
            text=chunk_text,
            document_id=document.id,
            chunk_index=i,
            chunk_type="paragraph",
            word_count=len(chunk_text.split()),
            character_count=len(chunk_text)
        )
        chunks.append(chunk)
    
    print(f"âœ… Created {len(chunks)} chunks (showing first 5)")
    for i, chunk in enumerate(chunks):
        preview = chunk.text[:100] + "..." if len(chunk.text) > 100 else chunk.text
        print(f"   Chunk {i+1}: {preview}")
    
    # Step 4: Knowledge Graph Extraction using existing pipeline âœ…
    print("\nğŸ•¸ï¸ Step 4: Knowledge Graph Extraction")
    try:
        entities, relationships = await run_kg_extraction(
            text=document.content[:2000],  # First 2000 chars for demo
            document_id=document.id
        )
        print(f"âœ… Extracted {len(entities)} entities and {len(relationships)} relationships")
        
        # Show some entities
        for entity in entities[:3]:
            print(f"   ğŸ·ï¸ {entity.name} ({entity.entity_type.value})")
            
        # Show some relationships  
        for rel in relationships[:3]:
            print(f"   ğŸ”— {rel['subject']} â†’ {rel['relation']} â†’ {rel['object']}")
            
    except Exception as e:
        print(f"âš ï¸ Knowledge extraction partially failed: {e}")
        entities, relationships = [], []
    
    # Step 5: Embedding Generation using existing EmbeddingService âœ…
    print("\nğŸ§® Step 5: Embedding Generation")
    try:
        embedding_service = EmbeddingService()
        
        # Generate embeddings for chunks
        for chunk in chunks:
            embedding = embedding_service.generate_embedding(chunk.text)
            chunk.embedding_vector = embedding
            print(f"âœ… Generated {len(embedding)}-dimensional embedding for chunk {chunk.chunk_index}")
            
    except Exception as e:
        print(f"âš ï¸ Embedding generation failed: {e}")
    
    # Step 6: Database Storage (commented out to avoid actual DB writes in demo)
    print("\nğŸ’¾ Step 6: Database Storage")
    print("ğŸ“ In a full implementation, this would:")
    print("   â€¢ Store Document in Neo4j + Weaviate via DocumentRepository")
    print("   â€¢ Store Chunks with embeddings via ChunkRepository")  
    print("   â€¢ Store Entities and Relationships in Neo4j")
    print("   â€¢ Update processing status to COMPLETED")
    
    # Show what would be stored
    print(f"\nğŸ“Š Pipeline Summary:")
    print(f"   ğŸ“„ Document: {document.title}")
    print(f"   ğŸ“ Content: {document.word_count:,} words")
    print(f"   ğŸª“ Chunks: {len(chunks)} created")
    print(f"   ğŸ·ï¸ Entities: {len(entities)} extracted") 
    print(f"   ğŸ”— Relationships: {len(relationships)} found")
    print(f"   ğŸ§® Embeddings: Generated for {len([c for c in chunks if hasattr(c, 'embedding_vector')])} chunks")
    print("\nâœ… Complete pipeline demonstration finished!")

def main():
    """Main demo function."""
    if len(sys.argv) != 2:
        print("Usage: python demo_document_ingestion.py <path_to_pdf>")
        print("\nExample:")
        print("  python demo_document_ingestion.py data/pdfs/republic.pdf")
        print("  python demo_document_ingestion.py data/pdfs/nicomachean_ethics.pdf")
        return
    
    pdf_path = sys.argv[1]
    
    if not Path(pdf_path).exists():
        print(f"âŒ File not found: {pdf_path}")
        print("Please ensure the PDF file exists and try again.")
        return
    
    # Run the demonstration
    asyncio.run(demonstrate_complete_pipeline(pdf_path))

if __name__ == "__main__":
    main()